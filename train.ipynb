{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pathlib\n",
    "from absl import logging, flags, app\n",
    "from absl.flags import FLAGS\n",
    "from utils.data_aug import get_dataset\n",
    "from utils.util import learning_rate_config, config_optimizer, add_regularization\n",
    "from utils.score import SegmentationMetric\n",
    "from utils.visualization import vis_segmentation\n",
    "from utils.loss import get_loss\n",
    "from models.model_summary import create_model\n",
    "import timeit\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import os\n",
    "from config import cfg\n",
    "from utils.util import set_device, ckpt_manager\n",
    "from utils.logger import create_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "set_device()\n",
    "output_dir = pathlib.Path(cfg.LOG_DIR)\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "logger = create_logger(name=__name__,\n",
    "                       output_dir=output_dir,\n",
    "                       filename='log.txt')\n",
    "# 数据集加载\n",
    "train_dataset = get_dataset(cfg.DATASET.TRAIN_DATA, cfg, is_training=True)\n",
    "val_dataset = get_dataset(cfg.DATASET.VAL_DATA, cfg)\n",
    "# for batch, (images, labels) in enumerate(train_dataset):\n",
    "#     for i in range(cfg.TRAIN.BATCH_SIZE):\n",
    "#         img = np.array(images[i, :, :, :] * 255).astype(np.int64)\n",
    "#         label = np.array(labels[i, :, :, 0]).astype(np.int64)\n",
    "#         vis_segmentation(img, label, label_names=cfg.DATASET.LABELS)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 模型搭建和损失函数配置\n",
    "model = create_model(cfg, name=cfg.MODEL_NAME, backbone=cfg.BACKBONE_NAME)\n",
    "model = add_regularization(model, tf.keras.regularizers.l2(cfg.LOSS.WEIGHT_DECAY))\n",
    "# model.summary()\n",
    "\n",
    "loss = get_loss(cfg, cfg.LOSS.TYPE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 优化器和学习率配置\n",
    "lr = tf.Variable(cfg.SCHEDULER.LR_INIT)\n",
    "learning_rate = learning_rate_config(cfg)\n",
    "\n",
    "# warmup策略\n",
    "def lr_with_warmup(global_steps):\n",
    "    lr_ = tf.cond(tf.less(global_steps, cfg.SCHEDULER.WARMUP_STEPS),\n",
    "                  lambda: cfg.SCHEDULER.LR_INIT * tf.cast((global_steps + 1) / cfg.SCHEDULER.WARMUP_STEPS, tf.float32),\n",
    "                  lambda: tf.maximum(learning_rate(global_steps - cfg.SCHEDULER.WARMUP_STEPS), cfg.SCHEDULER.LR_LOWER_BOUND))\n",
    "    return lr_\n",
    "\n",
    "optimizer = config_optimizer(cfg, learning_rate=lr)\n",
    "\n",
    "# 模型保存与恢复\n",
    "manager, ckpt = ckpt_manager(cfg, model, logger, optimizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 训练与验证静态图\n",
    "@tf.function\n",
    "def train_one_batch(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 1、计算模型输出和损失\n",
    "        pred_o = model(x, training=True)\n",
    "        # pred_o, l2, l3, l4, l5 = model(x, training=True)\n",
    "        regularization_loss_out = tf.reduce_sum(model.losses)\n",
    "        # seg_loss_out = loss(y, pred_o) + 0.1 * (loss(y, l2) + loss(y, l3) + loss(y, l4) + loss(y, l5))\n",
    "        seg_loss_out = loss(y, pred_o)\n",
    "        total_loss_out = seg_loss_out + regularization_loss_out\n",
    "    # 计算梯度以及更新梯度, 固定用法\n",
    "    grads = tape.gradient(total_loss_out, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return total_loss_out, seg_loss_out, pred_o\n",
    "\n",
    "@tf.function\n",
    "def val_one_batch(x, y):\n",
    "    # pred_o, _, _, _, _ = model(x, training=False)\n",
    "    pred_o = model(x, training=False)\n",
    "    return pred_o"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 记录器和评价指标\n",
    "summary_writer = tf.summary.create_file_writer(cfg.LOG_DIR)\n",
    "# tf.summary.trace_on(profiler=True)  # 开启Trace（可选）\n",
    "# 评价指标\n",
    "val_metric = SegmentationMetric(cfg.DATASET.N_CLASSES)\n",
    "train_metric = SegmentationMetric(cfg.DATASET.N_CLASSES)\n",
    "val_metric.reset()\n",
    "train_metric.reset()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# region # 迭代优化\n",
    "for _ in range(int(ckpt.step), cfg.TRAIN.EPOCHS):\n",
    "    # region # 训练集\n",
    "    ckpt.step.assign_add(1)\n",
    "    lr.assign(lr_with_warmup(optimizer.iterations))  # 必须使用assign才能改变optimizer的lr的值，否则，是个固定值\n",
    "\n",
    "    for batch, (images_batch, labels_batch) in tqdm(enumerate(train_dataset)):\n",
    "        total_loss, seg_loss, train_pred = train_one_batch(images_batch, labels_batch)\n",
    "        # 计算训练集精度\n",
    "        if int(ckpt.step) % 3 == 1:\n",
    "            train_out = np.argmax(train_pred, axis=-1)\n",
    "            for i in range(labels_batch.shape[0]):\n",
    "                train_label = np.array(labels_batch[i, :, :, 0]).astype(np.int64)\n",
    "                train_metric.addBatch(train_label, train_out[i, :, :])\n",
    "        # if epoch > 200:\n",
    "        with summary_writer.as_default():  # 指定记录器\n",
    "            tf.summary.scalar(\"train/total_losses\", total_loss, step=optimizer.iterations)  # 将当前损失函数的值写入记录器\n",
    "            tf.summary.scalar(\"train/segmentation_loss_loss\", seg_loss, step=optimizer.iterations)\n",
    "            tf.summary.scalar(\"train/learning_rate\", lr, step=optimizer.iterations)\n",
    "    # endregion\n",
    "\n",
    "    # region # 验证集\n",
    "    if int(ckpt.step) % 3 == 1:\n",
    "        # ----------------------------------------------验证集验证--------------------------------------------------------\n",
    "        for batch, (images_batch, labels_batch) in tqdm(enumerate(val_dataset)):\n",
    "            out = val_one_batch(images_batch, labels_batch)\n",
    "            out = np.squeeze(np.argmax(out, axis=-1))\n",
    "            labels_batch = np.array(labels_batch[0, :, :, 0]).astype(np.int64)\n",
    "            val_metric.addBatch(labels_batch, out)\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar(\"val_metric/mPA\", val_metric.meanPixelAccuracy(), step=int(ckpt.step))\n",
    "            tf.summary.scalar(\"val_metric/dice\", val_metric.dice(), step=int(ckpt.step))\n",
    "            tf.summary.scalar(\"val_metric/IoU1\", val_metric.IoU(1), step=int(ckpt.step))\n",
    "            tf.summary.scalar(\"val_metric/mIoU\", val_metric.mIoU(), step=int(ckpt.step))\n",
    "            tf.summary.scalar(\"train_metric/mPA\", train_metric.meanPixelAccuracy(), step=int(ckpt.step))\n",
    "            tf.summary.scalar(\"train_metric/mIoU\", train_metric.mIoU(), step=int(ckpt.step))\n",
    "            tf.summary.scalar(\"train_metric/dice\", train_metric.dice(), step=int(ckpt.step))\n",
    "            tf.summary.scalar(\"train_metric/IoU1\", train_metric.IoU(1), step=int(ckpt.step))\n",
    "            # VAL_PA = val_metric.meanPixelAccuracy()\n",
    "            logger.info('__EPOCH_{}__: TRAIN_mIoU: {:.5f}, TRAIN_mPA: {:.5f}, TRAIN_dice: {:.5f}; VAL_mIoU: {:.5f}, '\n",
    "                        'VAL_mPA: {:.5f}, VAL_dice: {:.5f}'\n",
    "                        .format(int(ckpt.step), train_metric.mIoU(), train_metric.meanPixelAccuracy(), train_metric.dice(),\n",
    "                                val_metric.mIoU(), val_metric.meanPixelAccuracy(), val_metric.dice()))\n",
    "        train_metric.reset()\n",
    "        val_metric.reset()\n",
    "    # endregion\n",
    "    # region # 模型保存\n",
    "    # 使用CheckpointManager保存模型参数到文件并自定义编号\n",
    "    manager.save(checkpoint_number=int(ckpt.step))\n",
    "    # model.save_weights(.FLAGSckpt_dir + '/BiseNetv2.tf')\n",
    "    # if TRA_PA >= 0.97 or VAL_PA >= 0.97:\n",
    "    #     os.mkdir(FLAGS.ckpt_dir + '//epoch_{}_val_{:.5f}_train_{:.5f}'.format(epoch, VAL_PA, TRA_PA))\n",
    "    #     tf.keras.models.save_model(\n",
    "    #         model, FLAGS.ckpt_dir + '//epoch_{}_val_{:.5f}_train_{:.5f}'.format(epoch, VAL_PA, TRA_PA))\n",
    "    # endregion\n",
    "# endregion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}